{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab70bd8c-e5d5-43d5-84e0-a713ff9379f3",
   "metadata": {},
   "source": [
    "## Generate CFs from WRF Data\n",
    "\n",
    "This file creates annual capacty factor files for the GCM in the folder it is pointed to, running wall time abt 20-25 minutes per year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974946ab-1022-4c07-9dd6-35dd85e3a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm = 'ec-earth3-veg_r1i1p1f1_ssp370_bc' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7dff3f-7299-4f10-899a-392ffae30e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d29e159-7a37-419e-b40b-528a264627af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "#specifiying power curves\n",
    "\n",
    "powercurves = pd.read_excel('/nfs/turbo/seas-mtcraig-climate/Martha_Research/EECS Project/wind_turbine_power_curves.xlsx',engine='openpyxl')\n",
    "\n",
    "powercurve_IECI = interpolate.interp1d(powercurves['Wind Speed'],powercurves['Composite IEC Class I'], bounds_error=False, fill_value=0)\n",
    "powercurve_IECII = interpolate.interp1d(powercurves['Wind Speed'],powercurves['Composite IEC Class II'])\n",
    "powercurve_IECIII = interpolate.interp1d(powercurves['Wind Speed'],powercurves['Composite IEC Class III'])\n",
    "\n",
    "#wind functions\n",
    "from wind_generation import calc_wind_power\n",
    "from calc_pv_potential import calc_pv_potential\n",
    "from calc_speed import calc_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6987fe-10e8-4962-8145-b490d459471b",
   "metadata": {},
   "source": [
    "#### Create Annaul CF Files for a GCM \n",
    "(change file path when run for different GCMs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1fe59f-c42f-4ec6-b2c4-477c097a7206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2061\n",
      "File saved to /nfs/turbo/seas-mtcraig-climate/WRFDownscaled/ec-earth3-veg_r1i1p1f1_ssp370_bc/Annual_Solar_Wind/Solar_Wind_CFs_2061.nc\n",
      "2765.38996219635\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store the processed datasets that will be combined\n",
    "processed_dataset_list = []\n",
    "\n",
    "years = [2061]\n",
    "\n",
    "# Walk through the main folder\n",
    "for YEAR in years:\n",
    "        loop_start_time = time.time()\n",
    "        # Construct the full file path\n",
    "        file_path = f\"/nfs/turbo/seas-mtcraig-climate/WRFDownscaled/{gcm}/{YEAR}/regrid_{YEAR}_ssp370_d02.nc\"\n",
    "        #Open the dataset \n",
    "        print(YEAR)\n",
    "        ds = xr.open_dataset(file_path)\n",
    "\n",
    "        #realign times \n",
    "        start_time = pd.Timestamp(f'{YEAR}-09-01 00:00')\n",
    "        times = pd.date_range(start=start_time, periods=len(ds.Times), freq='h')\n",
    "        ds['Times'] = times\n",
    "        #remove last hour of data because there is one hour of the next september \n",
    "        ds = ds.isel(Times=slice(None, -1))\n",
    "\n",
    "        #setting up wind data\n",
    "        wind_data = {}\n",
    "        #getting 100m wind speeds\n",
    "        wind_data['u100'] = (ds.U10*(100/10)**(1/7)).rename('u100')\n",
    "        wind_data['v100'] = (ds.V10*(100/10)**(1/7)).rename('v100')\n",
    "        #assigning attributes\n",
    "        wind_data['u100'].attrs.update(units= 'm s**-1', long_name = '100 metre U wind component')\n",
    "        wind_data['v100'].attrs.update(units= 'm s**-1', long_name = '100 metre V wind component')\n",
    "        uv100 = xr.merge([wind_data['u100'], wind_data['v100']])\n",
    "        #combing data to feed into calc wind power function \n",
    "        wind_speed = xr.map_blocks(calc_speed,uv100,args=['u100','v100']).compute()\n",
    "        wind_data['ps'] = ds.PSFC.rename('ps')\n",
    "        wind_data['tas'] = ds.T2.rename('tas')\n",
    "        wind_data['huss'] = ds.Q2.rename('huss')\n",
    "        wind_dset = xr.merge([wind_speed.to_dataset(name='100mWind'), wind_data['ps'], wind_data['tas'], wind_data['huss']])\n",
    "        wind_power = calc_wind_power(wind_dset,powercurve_IECI).compute()\n",
    "        WindPotential = wind_power/1500\n",
    "        WindPotential = WindPotential.rename('Wind_CF')\n",
    "        WindPotential = WindPotential.compute()\n",
    "        ds = ds.assign(Wind_CF = WindPotential)\n",
    "\n",
    "        #calculating solar cfs \n",
    "        solar_data = {}\n",
    "        solar_data['rsds'] = ds.SWDNB.rename('rsds')\n",
    "        #getting 10m wind speeds\n",
    "        wind_data['u10'] = ds.U10.rename('u10')\n",
    "        wind_data['v10'] = ds.V10.rename('v10')\n",
    "        #assigning attributes\n",
    "        wind_data['u10'].attrs.update(units= 'm s**-1', long_name = '10 metre U wind component')\n",
    "        wind_data['v10'].attrs.update(units= 'm s**-1', long_name = '10 metre V wind component')\n",
    "        uv10 = xr.merge([wind_data['u10'], wind_data['v10']])\n",
    "        #computing net wind speed\n",
    "        wind_speed10 = xr.map_blocks(calc_speed,uv10,args=['u10','v10']).compute()\n",
    "        #temp \n",
    "        solar_data['tas'] = ds.T2.rename('tas')\n",
    "        solar_dset = xr.merge([wind_speed10.to_dataset(name='surfWind'), solar_data['rsds'], solar_data['tas']])\n",
    "        #calculations\n",
    "        pv_pot = calc_pv_potential(solar_dset)\n",
    "        pv_pot = pv_pot.rename('Solar_CF')\n",
    "        pv_pot.attrs['units'] = 'None'\n",
    "        pv_pot.attrs['SS description'] = 'This is the solar potential of a site. Multiplying this by 1MW will give the ' \\\n",
    "                                         'electricity generation from a 1MW plant at different times and locations. '\n",
    "        ds = ds.assign(Solar_CF = pv_pot)\n",
    "\n",
    "        #drop other can capacity factors \n",
    "        ds = ds.drop_vars(['Q2', 'T2', 'PSFC', 'U10', 'V10', 'SWDNB', 'RUNSF', 'RUNSB'])\n",
    "\n",
    "        # Append the dataset with calculated variable to the list\n",
    "        processed_dataset_list.append(ds)\n",
    "\n",
    "        # Save the combined dataset to a new file\n",
    "        output_filename = f'/nfs/turbo/seas-mtcraig-climate/WRFDownscaled/{gcm}/Annual_Solar_Wind/Solar_Wind_CFs_{YEAR}.nc'  # Replace with your desired output file path\n",
    "        ds.to_netcdf(output_filename)\n",
    "        \n",
    "        print(f\"File saved to {output_filename}\")\n",
    "        loop_end_time = time.time()\n",
    "        elapsed_time = loop_end_time - loop_start_time\n",
    "        print(elapsed_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fcd76a-9dd0-454e-8aab-4b2c950ad9e1",
   "metadata": {},
   "source": [
    "#### Time Check for Process and Graphing Check if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26cba062-fd13-4a1a-b685-837908227020",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nfs/turbo/seas-mtcraig-climate/WRFDownscaled/ec-earth3-veg_r1i1p1f1_ssp370_bc/2019/regrid_2019_ssp370_d02.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/nfs/turbo/seas-mtcraig-climate/WRFDownscaled/ec-earth3-veg_r1i1p1f1_ssp370_bc/2019/regrid_2019_ssp370_d02.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '7384078a-b27d-4f5b-9ce4-8e3ccba9afb2']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/api.py:588\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    577\u001b[0m     decode_cf,\n\u001b[1;32m    578\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    585\u001b[0m )\n\u001b[1;32m    587\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 588\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    595\u001b[0m     backend_ds,\n\u001b[1;32m    596\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:645\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    626\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    644\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 645\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:408\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    402\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    403\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    404\u001b[0m )\n\u001b[1;32m    405\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    406\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    407\u001b[0m )\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:355\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:417\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:411\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    412\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/tensorflow-env/lib/python3.9/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2521\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2158\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nfs/turbo/seas-mtcraig-climate/WRFDownscaled/ec-earth3-veg_r1i1p1f1_ssp370_bc/2019/regrid_2019_ssp370_d02.nc'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = xr.open_dataset(f\"/nfs/turbo/seas-mtcraig-climate/WRFDownscaled/{gcm}/2019/regrid_2019_ssp370_d02.nc\")\n",
    "\n",
    "#realign times \n",
    "start_time = pd.Timestamp(f'2019-09-01 00:00')\n",
    "times = pd.date_range(start=start_time, periods=len(ds.Times), freq='h')\n",
    "ds['Times'] = times\n",
    "#remove last hour of data because there is one hour of the next september \n",
    "ds = ds.isel(Times=slice(None, -1))\n",
    "\n",
    "#setting up wind data\n",
    "wind_data = {}\n",
    "#getting 100m wind speeds\n",
    "wind_data['u100'] = (ds.U10*(100/10)**(1/7)).rename('u100')\n",
    "wind_data['v100'] = (ds.V10*(100/10)**(1/7)).rename('v100')\n",
    "#assigning attributes\n",
    "wind_data['u100'].attrs.update(units= 'm s**-1', long_name = '100 metre U wind component')\n",
    "wind_data['v100'].attrs.update(units= 'm s**-1', long_name = '100 metre V wind component')\n",
    "uv100 = xr.merge([wind_data['u100'], wind_data['v100']])\n",
    "#combing data to feed into calc wind power function \n",
    "wind_speed = xr.map_blocks(calc_speed,uv100,args=['u100','v100']).compute()\n",
    "wind_data['ps'] = ds.PSFC.rename('ps')\n",
    "wind_data['tas'] = ds.T2.rename('tas')\n",
    "wind_data['huss'] = ds.Q2.rename('huss')\n",
    "wind_dset = xr.merge([wind_speed.to_dataset(name='100mWind'), wind_data['ps'], wind_data['tas'], wind_data['huss']])\n",
    "wind_power = calc_wind_power(wind_dset,powercurve_IECI).compute()\n",
    "WindPotential = wind_power/1500\n",
    "WindPotential = WindPotential.rename('Wind_CF')\n",
    "WindPotential = WindPotential.compute()\n",
    "ds = ds.assign(Wind_CF = WindPotential)\n",
    "    \n",
    "#calculating solar cfs \n",
    "solar_data = {}\n",
    "solar_data['rsds'] = ds.SWDNB.rename('rsds')\n",
    "#getting 10m wind speeds\n",
    "wind_data['u10'] = ds.U10.rename('u10')\n",
    "wind_data['v10'] = ds.V10.rename('v10')\n",
    "#assigning attributes\n",
    "wind_data['u10'].attrs.update(units= 'm s**-1', long_name = '10 metre U wind component')\n",
    "wind_data['v10'].attrs.update(units= 'm s**-1', long_name = '10 metre V wind component')\n",
    "uv10 = xr.merge([wind_data['u10'], wind_data['v10']])\n",
    "#computing net wind speed\n",
    "wind_speed10 = xr.map_blocks(calc_speed,uv10,args=['u10','v10']).compute()\n",
    "#temp \n",
    "solar_data['tas'] = ds.T2.rename('tas')\n",
    "solar_dset = xr.merge([wind_speed10.to_dataset(name='surfWind'), solar_data['rsds'], solar_data['tas']])\n",
    "#calculations\n",
    "pv_pot = calc_pv_potential(solar_dset)\n",
    "pv_pot = pv_pot.rename('Solar_CF')\n",
    "pv_pot.attrs['units'] = 'None'\n",
    "pv_pot.attrs['SS description'] = 'This is the solar potential of a site. Multiplying this by 1MW will give the ' \\\n",
    "                                 'electricity generation from a 1MW plant at different times and locations. '\n",
    "ds = ds.assign(Solar_CF = pv_pot)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832dba-f7e0-4843-a66c-5ba9e5f6b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "windcf_slice = ds.windcf.isel(Times=1)\n",
    "\n",
    "# Set up the map projection and the figure using Plate Carrée\n",
    "proj = ccrs.PlateCarree()\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection=proj)\n",
    "\n",
    "# Add state borders from Natural Earth data\n",
    "states = cfeature.NaturalEarthFeature(category='cultural',\n",
    "                                      name='admin_1_states_provinces_lines',\n",
    "                                      scale='50m',\n",
    "                                      facecolor='none')\n",
    "ax.add_feature(states, edgecolor='black')\n",
    "\n",
    "# Plot the data on the map with the same Plate Carrée projection\n",
    "windcf_slice.plot(ax=ax, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Set the extent (adjust to your dataset's extents if needed)\n",
    "ax.set_extent([-125, -66.5, 20, 50], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add coastlines and borders for context\n",
    "ax.coastlines('10m', linewidth=0.8)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle='-', edgecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc95c4-f1e1-439c-a1d9-ca7b1c335348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
